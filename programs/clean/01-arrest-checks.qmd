---
author: Sarah Cohen
date: last-modified
title: Arrest table cleanup
subtitle: ""
format:
  scdefault-html:
    code-fold: show
    toc: false
execute:
  echo: fenced
  warning: true
  message: true
  error: true
  cache: false
  eval: true

engine: knitr
---

::: confidential
Not for publication
:::

The arrests table cleanup is done in this document. See the
documentation for details on why and how it ends up.

```{r}
#| label: setup
#| message: false
#| echo: fenced
#| error: true
#| code-fold: true

# assumes pacman has been installed already


# typical packages
pacman::p_load("tidyverse", "glue", "lubridate", "janitor", "readxl", "here", "googlesheets4")
here::i_am("programs/clean/01-arrest-checks.qmd")


#additional packages here
library(data.table)


rm (list=ls())

# when was this last picked up from the Deportation Data Project?
orig_xl_date <- mdy("10/14/2025")
#start with a blank slate


```

Data Last Updated: `{r} format(orig_xl_date, "%B %e, %Y")`

```{r}
#| label: convenience
#| code-fold: true

# convenience functions

# count numbr of rows with missing values in the specified column, optionally
# filtered by a condition represented in a string.
missing_nrow <- function(df, missing_column, optional_filter = NULL) {
  if (!is.null(optional_filter)) {
    df = df |> dplyr::filter(!!rlang::parse_expr(optional_filter))
  }

  filtered_df = df |> dplyr::filter(is.na(.data[[missing_column]]))
  nrow(filtered_df)
}

# Round to nearest N means  you can use 500, not just 1, 100, 1000, etc.
# Uses R rounding method, which would take 449.5 to 448, not 500, but I don't think
# it matters here. There won't be anything that odd.
round_nearest_n <- function (x,  n ) {
  rounded_value = round ( x / n) * n
  return (formatC(rounded_value, format = "d", big.mark = ",") )
}

# Takes an optional  list of objects, a string vector, and a file name
# to save an Rda with objects renamed from the original.
# example: saveit (  master_xl, lookup_tables, df_recoded ,
#                   string = c("arrest_original", 'arrest_lookups', "arrest_recoded") ,
#                   file = "/data/processed/arrests.Rda")
# the number of items in the vector have to match the number provided in the unlisted list

saveit <- function(..., string, file) {
        x <- list(...)
        names(x) <- string
        save(list=names(x), file=file, envir=list2env(x))
    }

```

## Read original Excel file from Deportation Data Project

The original Excel file has 19 columns that have to be imported with
variable types to take care of Zip codes, dates and missing values.
Reading everything as text has two problems: Floating point weirdness,
and dates converted to serial numbers. The data header is at row 7 and
the data starts row 8.

Two columns are dropped because they add no information:
`final_program_group` is always "ICE", `apprehension_county` is always
missing. This could change in the future.

I renamed the columns to a) lowercase everything; b) replace spaces and
special characters with underscores.[^1]

[^1]: It uses the `janitor::clean_names()` function, so it's always done
    the same way.

```{r}
#| label: get-arrest-rawdata
#| message: true
#| warning: true

var_types <- c ("guess", rep("text",9 ), "guess", "text", "text", "guess", "skip", "guess", rep("text", 3),
              rep("skip", 3), "text")

orig_xls <- read_excel(here(
                      "data/raw/ddp/2025-ICLI-00019_2024-ICFO-39357_ERO_AdminArrests_07312025.xlsx"
                      ),
                      skip=6,
                      col_types = var_types
                      )  |>
            clean_names() |>
            # I'll get the last arrest in the datast later.
            rowid_to_column(var="orig_row_id") |>

            # county is always empty
            select ( - c(apprehension_county, final_program_group))

glimpse (orig_xls)

```

## Basic cleanup

### Dates and missing values

1.  Convert dates and times: The date fields always have time elements,
    but only `apprehension_date` has meaningful time value. Create
    `apprehension_date_part` with the date portion and convert all of
    the others to date only.

2.  About
    `r missing_nrow( orig_xls, "unique_identifier") |> round_nearest_n(500)`
    rows have no unique identifier for people. For the purpose of
    finding and removing duplicates, create a `person_id`, which is the
    `unique_identifier` when it exists and a combination of arrest date,
    area of responsibility (or state or "UNKNOWN" when it doesn't exist)
    , and birth year. There's a pretty good chance that we could have
    multiple people arrested at the same time in the same place who are
    the same age but there's really not much other choice that I can
    see.

```{r }
#| label: fix-dates

master_xl <- orig_xls |>
  mutate ( apprehension_date_part =as_date(  apprehension_date),
            # convert datetime to date when it's always midnight
            across ( c ( final_order_date, departed_date), as_date),
          alternate_id =
                paste( format(apprehension_date_part, "%Y-%m-%d") ,
                  coalesce ( apprehension_aor, apprehension_state, "UNKNOWN" ) ,
                  coalesce ( sprintf("%04d", birth_year) , "0000" ),
                  sep="-") ,
    person_id = coalesce (unique_identifier,  alternate_id) ,
    .after = unique_identifier
    ) |>
  select ( orig_row_id, person_id, unique_identifier, apprehension_date, apprehension_date_part,
            apprehension_aor, apprehension_state, apprehension_site_landmark,
            final_program:gender) |>
  arrange ( person_id, apprehension_date, orig_row_id) |>
  group_by ( person_id, apprehension_date) |>
  fill ( apprehension_aor, apprehension_state) |>
  ungroup() |>
  arrange ( orig_row_id)


datelist <- master_xl |>
  summarize ( min_arrest_dt = min (apprehension_date_part, na.rm=TRUE),
              max_arrest_dt = max ( apprehension_date_part, na.rm=FALSE)

        ) |>
    pivot_longer (everything()) |>
    pull ( value ) |>
    format( "%B %e, %Y")



```

This data begins `r datelist[1]` and ends `r datelist[2]`, has
`r format(nrow(master_xl), big.mark=",", scientific=FALSE)`

```{r}
#| label: cleanupenv1
#| echo: false
#| message: false
#| warning: false

rm (datelist , var_types, orig_xls)


```

### Duplicates

The original sort order is by the original `unique_identifier` (missing
is at the end), then date/time of apprehension. It's unclear which comes
first when they're the same -- we don't know for sure whether to take
the first or last of these to create a unique item for that person at
that time. However, it appears that in most duplicate cases, there are a
series of entries that contain `case_status` of "E-Charging Document
Canceled by ICE", followed by something else. In those cases, the aor
and state is often missing. Because they're the same person at the same
time, I chose to fill in those missing values.

Don't confuse these with what appear to be true multiple arrests --
entries where the person was arrested in, say, 2023 and also 2025. I'm
leaving those in.

I'm using the data table method here because I know later on I'll need
the code for bigger tables.

```{r}
#| label: keep-unique

DT <- copy(master_xl)
setDT(DT)
setorderv(DT, c("person_id", "apprehension_date", "orig_row_id"))

# compute dupe_count for each group (adds column by reference, very fast)
DT[, dupe_count := .N, by = .(person_id, apprehension_date)]

# take the last row of each group (because we've ordered the table above)
deduped <- DT[, .SD[.N], by = .(person_id, apprehension_date)]

str(deduped)

```

::: callout-caution
This will still include duplicates by person because there were a lot of
people arrested in earlier years and again in 2025. Be sure to
distinct_n() rather than n() whenever it's important to count people by
group.
:::

## Create more general categories and apply lookups

The case information is often pretty specific, and we want to be able to
talk about broad groupings. I've documented where those groupings come
from in the \<documentation/docs-alldata.qmd\> -- they cross a lot of
the datasets. Here, I'm just applying those decisions to this set of
data.

Generally, the decisions were made based on the combinations I saw and
other peoples' publications or coding.

```{r}
#| label: get-lookups


## Applies all of the lookup tabes that I've finished

lookup_tables <- read_excel (here("documentation/background/master_data_dictionary_lookups.xlsx"),
                            sheet="lookups",
                            na= c("", "N/A") ) |>
                  select ( tables, varname, orig_value = `initial value`, recode_to = recode)


list_of_vars <- c("final_program", "apprehension_method", "case_status")


# create a list of named vectors to use in mutate statement

#THESE DEPEND ON OLD-STYLE %>% NOT |> --------------------------------
# it's way easier to do tmp functions and use the . placeholder than do it in
# base R and new syntax.

mapping_list <- lookup_tables %>%
    filter (!is.na( orig_value),
            varname %in% list_of_vars) %>%
    mutate(
      orig_value   = as.character(orig_value),
      # in case they're factors
      recode_to = as.character(recode_to)
    ) %>%
    split(.$varname) %>%
    map(~ setNames(.x$recode_to, .x$orig_value))

# this is still working on the data.table. Have to convert it back to the tibble to see the values.

df_recoded <- deduped %>%
  mutate(across(all_of(list_of_vars),
                ~ {

                  col_vec <- mapping_list[[cur_column()]]
                  #col_vec is a named vector for the current column
                  return_value <- col_vec[ as.character(.) ]
                  #res is the value in the current column use as the name for the vector value.
                },
                .names  =  "{.col}_recoded"
  )) |>
  as_tibble()

#----------------------------------------------------------------------------

glimpse(df_recoded)

```

### Saving the processed files

```{r}
#| label: save-rda-file

saveit (  master_xl, lookup_tables, df_recoded,
         string=c("arrests_original", "lookup_tables", "arrests_recoded"),
         file="data/processed/arrests.Rda")


```

## Diagnostics

It's still not entirely clear what these columns represent, so I wanted
to make some charts of their frequency and how they relate to one
another.

```{r}
#| label: add-libraries-for-analysis
pacman::p_load("reactable", "reactablefmtr", "DT", "janitor", "skimr", "GGalley")

```

1.  Is there a way to look at Kavanaugh stops from this?
